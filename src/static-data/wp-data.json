[
    {
       "id":3150,
       "date":"2019-11-14T10:44:43",
       "date_gmt":"2019-11-14T18:44:43",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=3150"
       },
       "modified":"2019-11-20T08:54:20",
       "modified_gmt":"2019-11-20T16:54:20",
       "slug":"sc19",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/sc19\/",
       "title":{
          "rendered":"SC19 Presentations by USC Researchers"
       },
       "content":{
          "rendered":"<p><strong>All presentations take place at the University of Southern California booth, #1825.<\/strong><\/p>\n<h3>Monday, November 18, 2019<\/h3>\n<p><strong>7:00 p.m.<\/strong><br \/>\nPhil Maechling, &#8220;SCEC Earthquake System Science Research&#8221;<\/p>\n<h3>Tuesday, November 19, 2019<\/h3>\n<p class=\"p1\"><strong>12:00 p.m.<\/strong><br \/>\nScott Callaghan, &#8220;2018-2019 CyberShake Study&#8221;<\/p>\n<p class=\"p1\"><strong>2:00 p.m.<\/strong><br \/>\nGeorge Papadimitriou, &#8220;Panorama 360: Online Performance Data Capture and Analysis for End-to-End Scientific Workflows&#8221;<\/p>\n<p class=\"p1\"><span class=\"s1\"><strong>3:00 p.m.<\/strong><br \/>\nPhil Maechling, &#8220;SCEC Earthquake System Science Research&#8221;<\/span><\/p>\n<p>&nbsp;<\/p>\n<h3>Wednesday, November 20, 2019<\/h3>\n<p class=\"p1\"><span class=\"s1\"><strong>10:00 a.m.<\/strong><br \/>\nPhil Maechling, &#8220;SCEC Earthquake System Science Research&#8221;<br \/>\n<\/span><\/p>\n<p><span class=\"s1\"><strong>11:00 a.m.<\/strong><br \/>\n<\/span>Krishna Giri Narra, &#8220;Slack Squeeze Coded Computing for Adaptive Straggler Mitigation&#8221;<\/p>\n<p class=\"p1\"><span class=\"s1\"><strong>2:00 p.m.<\/strong><br \/>\nRafael Ferreira da Silva, &#8220;An Overview of the Pegasus Workflow Management System&#8221;<br \/>\n<\/span><\/p>\n<p class=\"p1\"><span class=\"s1\"><strong>3:00 p.m.<\/strong><br \/>\nScott Callaghan, &#8220;2018-2019 CyberShake Study&#8221;<br \/>\n<\/span><\/p>\n<p class=\"p1\"><span class=\"s1\"><strong>4:00 p.m.<\/strong><br \/>\nRafael Ferreira da Silva, &#8220;WRENCH: Enabling Simulation-driven Development and Training of Cyberinfrastructure Systems&#8221;<br \/>\n<\/span><\/p>\n<h3>Thursday, November 21, 2019<\/h3>\n<p class=\"p1\"><span class=\"s1\"><strong>11:00 a.m.<\/strong><br \/>\nKrishna Giri Narra, &#8220;Slack Squeeze Coded Computing for Adaptive Straggler Mitigation&#8221;<\/span><\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>All presentations take place at the University of Southern California booth, #1825. Monday, November 18, 2019 7:00 p.m. Phil Maechling, &#8220;SCEC Earthquake System Science Research&#8221; Tuesday, November 19, 2019 12:00 p.m. Scott Callaghan, &#8220;2018-2019 CyberShake Study&#8221; 2:00 p.m. George Papadimitriou,&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/sc19\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":3285,
       "featured_media":0,
       "parent":0,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3150"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/3285"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=3150"
             }
          ],
          "version-history":[
             {
                "count":4,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3150\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3161,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3150\/revisions\/3161"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=3150"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":3049,
       "date":"2019-06-21T09:44:11",
       "date_gmt":"2019-06-21T16:44:11",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=3049"
       },
       "modified":"2019-07-02T14:15:38",
       "modified_gmt":"2019-07-02T21:15:38",
       "slug":"accessing-application",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/accounts\/accessing-application\/",
       "title":{
          "rendered":"Accessing the HPC Application Website"
       },
       "content":{
          "rendered":"<p>Occasionally, you may encounter issues accessing the HPC application website. The following information will assist you with the majority of the potential access issues. <\/p>\n<ol>\n<li>Access to <a href=\"https:\/\/hpc-web.usc.edu\/projects\/\" >https:\/\/hpc-web.usc.edu\/projects\/<\/a> is only possible from a USC IP address. To ensure that you are able to access the site, use USC&#8217;s Virtual Private Network (VPN) in the following cases:\n<ul>\n<li>When you are connecting to hpc-web.usc.edu from a non-USC IP address (e.g., your home wired or wireless network).<br \/>\n<\/p>\n<p><\/p>\n<li>When connecting through USC Secure Wireless fails (e.g., if the browser tries but cannot connect to hpc-web.usc.edu).<\/li>\n<\/ul>\n<p>For information on accessing the USC&#8217;s Virtual Private Network (VPN), see the <a title=\"VPN Overview\" href=\"http:\/\/itservices.usc.edu\/vpn\"  target=\"_blank\" rel=\"noopener noreferrer\">VPN Overview<\/a> page on the ITS website. You can download Cisco&#8217;s AnyConnect client (recommended) from this site.<\/p>\n<li>Access to the application page requires you to log in via Shibboleth. When you go to <a href=\"https:\/\/hpc-web.usc.edu\/projects\/\" >https:\/\/hpc-web.usc.edu\/projects\/<\/a>, you will need to enter your USC NetID and your USC NetID password. Your USC NetID is used to connect to services such as <a href=\"https:\/\/wd5.myworkday.com\/usc\/d\/home.htmld\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/wd5.myworkday.com']);\" target=\"_blank\" rel=\"noopener noreferrer\">Workday<\/a> and <a href=\"http:\/\/my.usc.edu\"  target=\"_blank\" rel=\"noopener noreferrer\">my.usc.edu<\/a>.<br \/>\n<\/p>\n<p>\n<strong>Note:<\/strong> Affiliates with iVIP accounts will not be able to access the application page as they are not eligible to apply for their own HPC account; however, they may be added as a member to an existing project account.\n<\/ol>\n<h3>Additional Assistance<\/h3>\n<p>If you need additional help, please send email to <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>Occasionally, you may encounter issues accessing the HPC application website. The following information will assist you with the majority of the potential access issues. Access to https:\/\/hpc-web.usc.edu\/projects\/ is only possible from a USC IP address. To ensure that you are&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/accounts\/accessing-application\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":26,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3049"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=3049"
             }
          ],
          "version-history":[
             {
                "count":6,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3049\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3079,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3049\/revisions\/3079"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/26"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=3049"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":3037,
       "date":"2019-06-21T09:25:58",
       "date_gmt":"2019-06-21T16:25:58",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=3037"
       },
       "modified":"2020-04-21T10:15:21",
       "modified_gmt":"2020-04-21T17:15:21",
       "slug":"before-applying",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/accounts\/before-applying\/",
       "title":{
          "rendered":"Before Applying for an HPC Account"
       },
       "content":{
          "rendered":"<p>Before applying for an HPC account, please read and understand the following information.<\/p>\n<h5>Understanding HPC policies<\/h5>\n<p>Before applying for an HPC account please <a href=\"https:\/\/hpcc.usc.edu\/resources\/accounts\/hpcc-policies\/\"  target=\"_blank\" rel=\"noopener noreferrer\"><strong>read and understand all HPC policies<\/strong><\/a>. HPC is a shared resource that serves the entire USC research community. An effort has been made to make its use as fair as possible; thus, there are usage limits for both computing and storage. It is expected that all researchers understand and abide by these policies. <strong>Compliance with these policies will be a factor in the evaluation of account renewals<\/strong>.<\/p>\n<h5>Acknowledging HPC Support<\/h5>\n<p>Any form of publication, including webpages, resulting from work done on HPC machines should include an acknowledgment similar to the following:<\/p>\n<p style=\"padding-left: 30px\">Computation for the work described in this paper was supported by the University of Southern California\u2019s Center for High-Performance Computing (<a href=\"https:\/\/hpcc.usc.edu\" >https:\/\/hpcc.usc.edu<\/a>).<\/p>\n<p>Example:<\/p>\n<p style=\"padding-left: 30px\">This research was supported by the Department of Energy, Office of Science, Basic Energy Sciences, Materials Science and Engineering Division, Grant # DE-FG02-04ER-46130. The simulations were performed at the Center for High-Performance Computing at the University of Southern California (<a href=\"https:\/\/hpcc.usc.edu\" >https:\/\/hpcc.usc.edu<\/a>).<\/p>\n<h5>Providing Grant and Publication Information<\/h5>\n<p>To help measure return on investment, HPC requires that you submit information about grants and other funding that support HPC research and on publications that have been made possible through the use of HPC. Grants and published papers acknowledging HPC should be submitted through the <a href=\"https:\/\/hpc-web.usc.edu\/projects\/\"  target=\"_blank\" rel=\"noopener noreferrer\">HPC project website<\/a>. Be sure to include complete publication information and a link to or copy of the actual publication (URL, PDF, PS), if possible. Please indicate if there are any restrictions on publication. <strong>Compliance with this policy will be a factor in the evaluation of account renewals<\/strong>.<\/p>\n<h3>Getting Help<\/h3>\n<p>If you have any questions regarding expectations for HPC account holders, please send email to <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>Before applying for an HPC account, please read and understand the following information. Understanding HPC policies Before applying for an HPC account please read and understand all HPC policies. HPC is a shared resource that serves the entire USC research&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/accounts\/before-applying\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":26,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3037"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=3037"
             }
          ],
          "version-history":[
             {
                "count":9,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3037\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3223,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3037\/revisions\/3223"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/26"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=3037"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":3033,
       "date":"2019-06-21T09:23:57",
       "date_gmt":"2019-06-21T16:23:57",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=3033"
       },
       "modified":"2019-07-02T09:38:30",
       "modified_gmt":"2019-07-02T16:38:30",
       "slug":"draft-accounts",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/draft-accounts\/",
       "title":{
          "rendered":"DRAFT: HPC Accounts"
       },
       "content":{
          "rendered":"",
          "protected":true
       },
       "excerpt":{
          "rendered":"",
          "protected":true
       },
       "author":111,
       "featured_media":0,
       "parent":0,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3033"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=3033"
             }
          ],
          "version-history":[
             {
                "count":7,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3033\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3069,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/3033\/revisions\/3069"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=3033"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":2988,
       "date":"2019-06-03T15:25:44",
       "date_gmt":"2019-06-03T22:25:44",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=2988"
       },
       "modified":"2019-06-10T09:03:57",
       "modified_gmt":"2019-06-10T16:03:57",
       "slug":"parallel",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/documentation\/r\/parallel\/",
       "title":{
          "rendered":"Parallel R on HPC"
       },
       "content":{
          "rendered":"<p>You may have times where you need to run a single program on multiple data sets. There are a number of ways to do this&mdash;the simplest way may be to use Slurm&#8217;s <strong>srun<\/strong> command with the <strong>&ndash;&ndash;multi-prog<\/strong> option. <\/p>\n<p>For example, below we will create 4 data files, each with one line:<\/p>\n<pre>\r\n$ ls mydata*\r\nmydata1\r\nmydata2\r\nmydata3\r\nmydata4\r\n<\/pre>\n<p>The first line of <strong>mydata1<\/strong> will be 111, that of <strong>mydata2<\/strong> will be 222, and so on.<\/p>\n<pre>\r\n$ cat mydata*\r\n111\r\n222\r\n333\r\n444\r\n<\/pre>\n<p>Paste the following into a file named <strong>myjob.slurm<\/strong>. (For this exercise, an example from <a href=\"https:\/\/www.tchpc.tcd.ie\/node\/167\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.tchpc.tcd.ie']);\">https:\/\/www.tchpc.tcd.ie\/node\/167<\/a> was used and modified to run on HPC.)<\/p>\n<pre>\r\n#!\/bin\/bash\r\n\r\n#SBATCH --ntasks=12 \r\n#SBATCH --nodes=1\r\n#SBATCH --time 00:10:00\r\n#SBATCH --job_name=rtest\r\n#SBATCH --export=none\r\n#SBATCH --mem-per-cpu=2g\r\n\r\nsrun --label --multi-prog myjob.config\r\n<\/pre>\n<p>Paste the following lines into a file named <strong>myjob.config<\/strong>. This tells Slurm what tasks to run on each of the CPUs that will be allocated. (On HPC, cpus-per-task=1 by default.) <\/p>\n<p><strong>NOTE:<\/strong> On HPC, the <strong>&ndash;&ndash;multi-prog<\/strong> option requires that the number of tasks in the configuration list must exactly equal the number of tasks allocated. The <strong>%t<\/strong> and <strong>%o<\/strong> will be populated with the task number and task offset, respectively. <\/p>\n<p>0-3   \/usr\/bin\/hostname<br \/>\n4,5   \/usr\/bin\/echo            task:%t<br \/>\n6     \/usr\/bin\/echo            task:%t-%o<br \/>\n7     \/usr\/bin\/echo            task:%o<br \/>\n8     \/usr\/bin\/cat data1<br \/>\n9     \/usr\/bin\/cat data2<br \/>\n10    \/usr\/bin\/cat data3<br \/>\n11    \/usr\/bin\/cat data4<br \/>\n12    \/usr\/bin\/hostname<\/p>\n<p>While the syntax &#8220;0-3&#8221; and &#8220;4,5&#8221; can be used as shown, you will more likely use something like the last four lines to run a program with different data sets. If you wanted <strong>task1<\/strong> to process <strong>data1<\/strong>, etc., you could use:<\/p>\n<pre>1-4 \/usr\/bin\/cat data%t<\/pre>\n<p>Once you have your file correctly configured, it&#8217;s time to submit the job:<\/p>\n<pre>sbatch myjob.slurm<\/pre>\n<p>When the jobs ends, you should see output similar to the following:<\/p>\n<pre>\r\n---------- Begin SLURM Prolog ----------\r\nJob ID:        2542141\r\nUsername:      erinshaw\r\nAccountname:   lc_hpcc\r\nName:          rtest\r\nPartition:     quick\r\nNodelist:      hpc1118\r\nTasksPerNode:  16\r\nCPUsPerTask:   Default[1]\r\nTMPDIR:        \/tmp\/2542141.quick\r\nSCRATCHDIR:    \/staging\/scratch\/2542141\r\nCluster:       uschpc\r\nHSDA Account:  false\r\n---------- 2019-01-14 10:07:00 ---------\r\n 3: hpc1118\r\n 6: task:6-0\r\n 5: task:5\r\n 2: hpc1118\r\n 7: task:0\r\n 4: task:4\r\n 0: hpc1118\r\n 8: 111\r\n 9: 222\r\n11: 444\r\n 1: hpc1118\r\n10: 333\r\n<\/pre>\n<p>When ordered, tasks 8-11 look like this:<\/p>\n<pre>\r\n 8: 111\r\n 9: 222\r\n10: 333\r\n11: 444\r\n<\/pre>\n<h2>Parallel R Packages<\/h2>\n<p>Several parallel packages have been developed for R, including <strong>parallel<\/strong>, <strong>doParallel<\/strong>, <strong>rslurm<\/strong>, and <strong>BiocParallel<\/strong>. See the <a href=\"https:\/\/cran.r-project.org\/web\/views\/HighPerformanceComputing.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/cran.r-project.org']);\" target=\"_blank\" rel=\"noopener noreferrer\">CRAN HPC Page<\/a> for a full description of options for running R in parallel. <\/p>\n<p><strong>NOTE:<\/strong> You must utilize a standard head node (e.g., hpc-login3.usc.edu) to install R packages. To test parallel code, you must be on a compute node.<\/p>\n<p>Here are some examples of running R on HPC using different parallel packages. For the examples, we&#8217;ll use the birthday function from the <a href=\"https:\/\/www.r-bloggers.com\/are-you-doing-parallel-computations-in-r-then-use-biocparallel\/ \" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.r-bloggers.com']);\">BiocParallel<\/a> blog post by Leonardo Collado-Torres.<\/p>\n<p>To get started with this tutorial, copy and paste the following lines into R:<\/p>\n<pre>\r\n> birthday <- function(n) {\r\n+     m <- 10000\r\n+     x <- numeric(m)\r\n+     for(i in seq_len(m)) {\r\n+         b <- sample(seq_len(365), n, replace = TRUE)\r\n+         x[i] <- ifelse(length(unique(b)) == n, 0, 1)\r\n+     }\r\n+     mean(x)\r\n+ }\r\n<\/pre>\n<p>You can use <strong>lapply()<\/strong> or a for loop to calculate the results. In the example below, we use <strong>lapply()<\/strong>:<\/p>\n<pre>\r\nsystem.time( lapply(seq_len(100), birthday) )\r\n<\/pre>\n<p>We can now test the birthday function with some parallel options for R. While you are downloading and installing these libraries (<strong>parallel, rslurm, BiocParallel<\/strong>), you should also install the package <strong>foreach<\/strong> for testing purposes.<\/p>\n<p><!-- Not available for R 3.5.0 Package: doparallel\nSee the manual for R's parallel library at <a href=\"https:\/\/cran.r-project.org\/web\/packages\/doParallel\/doParallel.pdf\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/cran.r-project.org']);\">Package 'doParallel'<\/a> and especially <a href=\"https:\/\/cran.r-project.org\/web\/packages\/doParallel\/vignettes\/gettingstartedParallel.pdf\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/cran.r-project.org']);\">Getting Started with doParallel and foreach<\/a>. --><\/p>\n<h3>Package: parallel<\/h3>\n<p>See the following R manual for R's parallel library:  <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/parallel\/doc\/parallel.pdf\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/stat.ethz.ch']);\">Package 'parallel'<\/a>.  <\/p>\n<pre>\r\n## 20(x2) cores detected on hpc4515\r\n> library(parallel)\r\n# Find out how many cores are available (if you don't already know)\r\n> detectCores()\r\n[1] 20\r\n# Create cluster with desired number of cores\r\ncl <- makeCluster(3)\r\n# Find out how many cores are being used\r\n> detectCores()\r\n[1] 20\r\n> print(\"Hello World!\")\r\n\"Hello World!\"\r\n> stopCluster(cl)\r\n<\/pre>\n<h3>Package: rslurm<\/h3>\n<p>See the manual for R's rslurm library at <a href=\"https:\/\/cran.r-project.org\/web\/packages\/rslurm\/rslurm.pdf\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/cran.r-project.org']);\">Package 'rslurm'<\/a>. The following example is from <a href=\"https:\/\/cran.r-project.org\/web\/packages\/rslurm\/vignettes\/rslurm.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/cran.r-project.org']);\" target=\"_blank\" rel=\"noopener noreferrer\">Parallelize R code on a Slurm cluster<\/a>, and was modified for use with our birthday function test script.<\/p>\n<pre>\r\n> library(rslurm)\r\n> pars <- data.frame(n=seq_len(100))\r\n> sjob <- slurm_apply(birthday, pars, jobname='myjob',cpus=8,submit=TRUE)\r\nSubmitted batch job 2398557\r\n> list.files('_rslurm_myjob', 'results')\r\n[1] \"results_0.RDS\" \"results_1.RDS\"\r\n> res <- get_slurm_out(sjob, outtype='table')\r\n> res\r\n<\/pre>\n<p>Output will be placed in a subdirectory named \"_rslurm_myjob\":<\/p>\n<pre>\r\n$ ls _rslurm_myjob\r\nf.RDS  params.RDS  results_0.RDS  results_1.RDS  \r\nslurm_0.out  slurm_1.out  slurm_run.R  submit.sh\r\n<\/pre>\n<h3>Package: BiocParallel<\/h3>\n<p>See the manual for R's BiocParallel library at <a href=\"https:\/\/bioconductor.org\/packages\/release\/bioc\/manuals\/BiocParallel\/man\/BiocParallel.pdf\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/bioconductor.org']);\">Package 'BiocParallel'<\/a>. This example is from the <a href=\"https:\/\/www.r-bloggers.com\/are-you-doing-parallel-computations-in-r-then-use-biocparallel\/ \" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.r-bloggers.com']);\">BiocParallel<\/a> blog post by Leonardo Collado-Torres. <\/p>\n<p>Once you have loaded the BiocParallel library, request a compute node with 8 CPUs (e.g., <strong>&ndash;&ndash;ntasks=8<\/strong>), run R, and try the following: <\/p>\n<pre>\r\n## Load library\r\n> library('BiocParallel')\r\n\r\n## Test the birthday() function\r\n> system.time( lapply(seq_len(100), birthday) )\r\n   user  system elapsed \r\n 33.394   0.153  33.600 \r\n\r\n## The results of registered() will depend on the compute node you are allocated.\r\n> registered()\r\n$MulticoreParam\r\nclass: MulticoreParam\r\n  bpisup: FALSE; bpnworkers: 22; bptasks: 0; bpjobname: BPJOB\r\n  bplog: FALSE; bpthreshold: INFO; bpstopOnError: TRUE\r\n  bptimeout: 2592000; bpprogressbar: FALSE; bpexportglobals: TRUE\r\n  bpRNGseed: \r\n  bplogdir: NA\r\n  bpresultdir: NA\r\n  cluster type: FORK\r\n\r\n$SnowParam\r\nclass: SnowParam\r\n  bpisup: FALSE; bpnworkers: 22; bptasks: 0; bpjobname: BPJOB\r\n  bplog: FALSE; bpthreshold: INFO; bpstopOnError: TRUE\r\n  bptimeout: 2592000; bpprogressbar: FALSE; bpexportglobals: TRUE\r\n  bpRNGseed: \r\n  bplogdir: NA\r\n  bpresultdir: NA\r\n  cluster type: SOCK\r\n\r\n$SerialParam\r\nclass: SerialParam\r\n  bpisup: TRUE; bpnworkers: 1; bptasks: 0; bpjobname: BPJOB\r\n  bplog: FALSE; bpthreshold: INFO; bpstopOnError: TRUE\r\n  bptimeout: 2592000; bpprogressbar: FALSE; bpexportglobals: TRUE\r\n  bplogdir: NA\r\n\r\n## Try Mulitcore\r\n> system.time( y.snow <- bplapply(1:10, birthday, BPPARAM = MulticoreParam(workers = 4)) )\r\n   user  system elapsed \r\n  0.034   0.186   1.450 \r\n\r\n## Try Snow\r\n> system.time( y.snow <- bplapply(1:10, birthday, BPPARAM = SnowParam(workers = 4)) )\r\n   user  system elapsed \r\n  0.049   0.043   5.238 \r\n\r\n## Try Serial\r\n> system.time( y.snow <- bplapply(1:10, birthday, BPPARAM = SerialParam()) )\r\n   user  system elapsed \r\n  2.957   0.037   2.996 \r\n<\/pre>\n<p>(See <a href=\"https:\/\/hpccpreview.usc.edu\/support\/documentation\/how-to-run-r-on-hpc\/?preview_id=2800&#038;preview_nonce=1278aee8ec&#038;_thumbnail_id=-1&#038;preview=true\" >Running R on HPC page<\/a> for more details on how to do this.) <\/p>\n<h2>OMP_NUM_THREADS<\/h2>\n<p>Parallel libraries with OpenMP (e.g., OpenBLAS, Intel MKL) support use of the environment variable <strong>OMP_NUM_THREADS<\/strong> to set the maximum number of threads to use for parallel processing. We recommended that you set this variable to 1.<\/p>\n<pre>export OMP_NUM_THREADS=1<\/pre>\n<p>For optimization, the value can be set automatically using a Slurm environment variable (e.g., <strong>$SLURM_CPUS_ON_NODE<\/strong>, <strong>$SLURM_CPUS_PER_TASK<\/strong>, or <strong>$SLURM_TASKS_PER_NODE\/$SLURM_JOBS_CPUS_PER_NODE<\/strong>). Some experimentation may be needed.<\/p>\n<h2>Getting Help<\/h2>\n<p>For assistance with running parallel jobs on HPC, see our <a href=\"\/getting-help\">Getting Help<\/a> page or send an email to <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a>.<\/p>\n<h3>USC Biostatistics Resources<\/h3>\n<p>Researchers from USC's Biostatistics community have developed addtional training, code, and documentation for running parallel R on HPC. These resources are available at <a href=\"https:\/\/github.com\/USCbiostats\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\" target=\"_blank\" rel=\"noopener noreferrer\">https:\/\/github.com\/USCbiostats<\/a>.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/USCbiostats\/hpc-with-r\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\" target=\"_blank\" rel=\"noopener noreferrer\">https:\/\/github.com\/USCbiostats\/hpc-with-r<\/a>\n<li><a href=\"https:\/\/github.com\/USCbiostats\/rslurm\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\" target=\"_blank\" rel=\"noopener noreferrer\">https:\/\/github.com\/USCbiostats\/rslurm<\/a>\n<li><a href=\"https:\/\/github.com\/USCbiostats\/software-dev\/tree\/master\/happy_scientist\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\" target=\"_blank\" rel=\"noopener noreferrer\">https:\/\/github.com\/USCbiostats\/software-dev\/tree\/master\/happy_scientist<\/a>\n<\/ul>\n<p><strong>Questions about these resources should be addressed to <a href=\"https:\/\/github.com\/orgs\/USCbiostats\/people\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\">USC Biostatistics' GitHub developers<\/a> and not to HPC.<\/strong><\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>You may have times where you need to run a single program on multiple data sets. There are a number of ways to do this&mdash;the simplest way may be to use Slurm&#8217;s srun command with the &ndash;&ndash;multi-prog option. For example,&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/documentation\/r\/parallel\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":2973,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2988"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=2988"
             }
          ],
          "version-history":[
             {
                "count":10,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2988\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3032,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2988\/revisions\/3032"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2973"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=2988"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":2983,
       "date":"2019-06-03T15:13:24",
       "date_gmt":"2019-06-03T22:13:24",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=2983"
       },
       "modified":"2019-06-07T15:57:20",
       "modified_gmt":"2019-06-07T22:57:20",
       "slug":"q-chem",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/documentation\/q-chem\/",
       "title":{
          "rendered":"Q-Chem on HPC"
       },
       "content":{
          "rendered":"<p><a href=\"http:\/\/www.q-chem.com\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.q-chem.com']);\">Q-Chem<\/a> is a software package for analyzing &#8220;molecular structures, reactivities, and vibrational, electronic, and NMR spectra&#8221; that is available to HPC researchers by permission. To request access to this software on HPC, please email <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a> <\/p>\n<p><strong>NOTE:<\/strong> Due to licensing restrictions with Gaussian, HPC users may not use Q-Chem and Guassian concurrently. Please email <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a> to switch access.  <\/p>\n<p>This page will help you set up Q-Chem on your HPC account.<\/p>\n<h2>Setting Up Your Q-Chem Environment<\/h2>\n<p>Q-Chem requires a number of environment variables to run properly on HPC. Some environment settings are configured automatically when you run Q-Chem&#8217;s HPC setup script:<\/p>\n<pre>\r\n$ source \/usr\/usc\/qchem\/5.1\/setup.sh\r\n$ env | grep QC\r\nQC=\/auto\/usc\/qchem\/5.1\r\nQCPROG_S=\/auto\/usc\/qchem\/5.1\/exe\/qcprog.exe_s\r\nQCRSH=ssh\r\nQCPLATFORM=LINUX_Ix86_64\r\nQCAUX=\/auto\/usc\/qchem\/5.1\/qcaux\r\nQCPROG=\/auto\/usc\/qchem\/5.1\/exe\/qcprog.exe\r\nQCMPI=mpich\r\n<\/pre>\n<p>Other variables that you will need to configure include the scratch directory, the machine file, and a location for your temporary files. We also recommend that you configure your Q-Chem job for checkpointing.<\/p>\n<h3>Scratch Directory<\/h3>\n<p>Q-Chem requires that you set the environment variable <strong>QCSCRATCH<\/strong>. You should set it to the directory that Q-Chem will use for storing temporary files when running a job.  <\/p>\n<p>One suggestion is to set <strong>QCSCRATCH<\/strong> to <strong>$SCRATCHDIR<\/strong>. <strong>SCRATCHDIR<\/strong> is the temporary directory that Slurm creates for its own temporary job files.  <\/p>\n<pre>\r\nexport QCSCRATCH=$SCRATCHDIR\r\n<\/pre>\n<h3>Machine File<\/h3>\n<p>Q-Chem also requires that you set the environment variable <strong>QCMACHINEFILE<\/strong>. This should be set to a text file that contains a list of all hostnames that your job has access to. <\/p>\n<p>This information is only relevant for the duration of the job, so we suggest you place the file in <strong>$QCSCRATCH<strong>.<\/p>\n<pre>\r\nexport QCMACHINEFILE=$QCSCRATCH\/<em>machinefile<\/em>\r\n<\/pre>\n<p>You can create <strong><em>machinefile<\/em><\/strong> by redirecting the output of the Slurm command <strong>scontrol show hostname<\/strong> to <strong>$QCMACHINEFILE<\/strong> which will display the machines being used for the current job. <\/p>\n<pre>\r\nscontrol show hostname &gt; <em>machinefile<\/em>\r\n<\/pre>\n<p>Each Q-Chem job may have different requirements for the machine file; it may be necessary to modify the output of the <strong>scontrol<\/strong> command depending on your job&#8217;s requirements. <\/p>\n<h3>Saving Temporary Files<\/h3>\n<p>If you wish to save KEY temporary files produced by Q-Chem during a run, you can run the program with a save directory argument. By default, Q-Chem will save the files to <strong>$QCSCRATCH\/<em>savedir<\/em><\/strong>. <\/p>\n<p><strong>NOTE:<\/strong> Slurm&#8217;s <strong>$SCRATCHDIR<\/strong> is deleted at the end of each job. If you have set <strong>QCSCRATCH<\/strong> to <strong>$SCRATCHDIR<\/strong>, you MUST copy the files to a persistent location before the end of your job.<\/p>\n<pre>\r\nexport QCSCRATCH=$SCRATCHDIR\r\nqchem data.in data.out data.savedir \r\ncp -r $QCSCRATCH\/data.savedir .\r\n<\/pre>\n<p>Alternatively, you can set <strong>QCSCRATCH<\/strong> to a persistent location. Do this if you want to save ALL temporary files (normally not required) using the <strong>qchem  &ndash;save<\/strong> option. <\/p>\n<pre>\r\nexport QCSCRATCH=\/staging\/tt1\/ttrojan\r\nqchem -save data.in data.out data.savedir \r\n<\/pre>\n<h3>Checkpointing Option<\/h3>\n<p>HPC recommends <a href=\"https:\/\/hpcc.usc.edu\/support\/documentation\/checkpointing\/\" >checkpointing<\/a> your Q-Chem job, especially if you plan to use the <a href=\"https:\/\/hpcc.usc.edu\/support\/documentation\/scavenge\/\" >scavenge partition<\/a>. A Q-Chem checkpoint file can be requested by setting GUI = 2 in the <a href=\"http:\/\/www.q-chem.com\/qchem-website\/manual\/qchem51_manual\/sect0057.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.q-chem.com']);\">$rem section<\/a> of the input file. The checkpoint file name is determined by the <strong>$GUIFILE<\/strong> environment variable.<\/p>\n<pre>\r\nexport GUIFILE=data.fchk\r\n<\/pre>\n<h2>Running a Q-Chem Job<\/h2>\n<p>The most basic way to run Q-Chem on a single processor (default) would be to run a command in the following form:<\/p>\n<pre>\r\nqchem data.in data.out data.savedir &\r\n<\/pre>\n<p>where <strong><em>data.in<\/em><\/strong> is the name of your Q-Chem input file, <strong><em>data.out<\/em><\/strong> is the name of the file where Q-Chem will place its output, and <strong><em>data.dir<\/em><\/strong> is the directory where Q-Chem will save key temporary files.<\/p>\n<p><strong>NOTE:<\/strong> If an output file already exists, it will be overwritten. <\/p>\n<p>The <strong>&#038;<\/strong> places the job into the background so that you may continue to work in the current shell.<\/p>\n<p>See the <a href=\"http:\/\/www.q-chem.com\/qchem-website\/manual\/qchem44_manual\/sect-running.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.q-chem.com']);\">Running Q-Chem<\/a> manual page on the Q-Chem website for more details. <\/p>\n<h3>Command-Line Options<\/h3>\n<p>To run Q-Chem in parallel, the number of the number of threads, <strong><em>nt<\/em>, (assumes 1 thread=1 CPU core) and the number of <\/strong>MPI processes (CPUs), <strong><em>np<\/em><\/strong>, may need be specified. If <em>np<\/em> is not specified, Q-Chem will default to running locally on a single node.<\/p>\n<p>The default command, above, is equivalent to:<\/p>\n<pre>\r\nqchem -nt 1 data.in data.out data.savedir &\r\n<\/pre>\n<p>When <strong><em>-np<\/em><\/strong> is not given, Q-Chem will default to running locally on a single node.<\/p>\n<h3>Parallel Options<\/h3>\n<p>Q-Chem supports multiple methods to run in parallel, as described in the table below.<\/p>\n<table>\n<thead>\n<tr>\n<th>Type<\/th>\n<th>Description<\/th>\n<th>Slurm Requirements<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>OpenMP<\/td>\n<td>Multiple threads (CPUs)<\/td>\n<td>Single task<br \/>(Multiple CPUs per task)<\/td>\n<\/tr>\n<tr>\n<td>MPI<\/td>\n<td>Multiple MPI processes<br \/>(One CPU each)<\/td>\n<td>Multiple tasks<br \/>(One CPU per task)<\/td>\n<\/tr>\n<tr>\n<td>MPI+OpenMP (Hybrid)<\/td>\n<td>Multiple MPI processes<br \/>(Multiple CPUs each)<\/td>\n<td>Multiple tasks<br \/>(Multiple CPUs per task)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<h4>Example Q-Chem Script<\/h4>\n<p>There are a number of sample Q-Chem jobs available in <strong>\/usr\/usc\/qchem\/<<em>version<\/em>>\/samples<\/strong>. You can use these sample input files as a starting place to building your own files.<\/p>\n<p>To use a sample directory, copy it to your project directory and run it there. Compare your output with the reference files (<strong>.out<\/strong>) of the same name.<\/p>\n<pre>\r\ncp -r \/auto\/usc\/qchem\/5.1\/samples\/freq .\r\n<\/pre>\n<p>Below is a template script that can be used to run Q-Chem on either a single- or multi-node job using a sample input file. <\/p>\n<pre>\r\n#!\/bin\/bash\r\n#SBATCH --export=none\r\n#SBATCH --ntasks=2\r\n#SBATCH --cpus-per-task=12\r\n#SBATCH --time=1:00:00\r\n#SBATCH --mem-per-cpu=2GB\r\n#SBATCH --job-name=qchem\r\n\r\nsource \/usr\/usc\/qchem\/5.1\/setup.sh\r\n\r\n# Define the scratch dir for savedir and machine file\r\nexport QCSCRATCH=$SCRATCHDIR\r\nexport QCMACHINEFILE=$SCRATCHDIR\/machinefile\r\n\r\n# Create machine file for this job\r\nscontrol show hostname > $QCMACHINEFILE\r\n\r\n# Sample files were copied to this directory\r\ncd $SLURM_SUBMIT_DIR\/freq\r\n\r\n# $SLURM_NTASKS will take the value provided by --ntasks option\r\n# $SLURM_CPUS_PER_TASK will take the value provided by --cpus-per-task option\r\n\r\nqchem -np $SLURM_NTASKS -nt $SLURM_CPUS_PER_TASK FREQ_water.in FREQ_water.out FREQ_water\r\n\r\n# Save key temporary files for curiosity\r\ncp -r $QCSCRATCH\/FREQ_water .\r\n<\/pre>\n<h2>Getting Help<\/h2>\n<p>For assistance with running Q-Chem on HPC, see our <a href=\"\/getting-help\">Getting Help<\/a> page or send an email to <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>Q-Chem is a software package for analyzing &#8220;molecular structures, reactivities, and vibrational, electronic, and NMR spectra&#8221; that is available to HPC researchers by permission. To request access to this software on HPC, please email hpc@usc.edu NOTE: Due to licensing restrictions&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/documentation\/q-chem\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":44,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2983"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=2983"
             }
          ],
          "version-history":[
             {
                "count":8,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2983\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3024,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2983\/revisions\/3024"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/44"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=2983"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":2973,
       "date":"2019-06-03T10:12:47",
       "date_gmt":"2019-06-03T17:12:47",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=2973"
       },
       "modified":"2019-06-07T15:50:16",
       "modified_gmt":"2019-06-07T22:50:16",
       "slug":"r",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/documentation\/r\/",
       "title":{
          "rendered":"R on HPC"
       },
       "content":{
          "rendered":"<p>The following is meant to be a brief guide to running R jobs on HPC. For more details, please see the <a href=\"https:\/\/hpcc.usc.edu\/gettingstarted\/\" >New User Guide<\/a> or <a href=\"https:\/\/hpcc.usc.edu\/support\/documentation\/slurm\/\" >Running a job on HPC using Slurm<\/a>.<\/p>\n<h3>Initialization Step (Before Installing R Packages)<\/h3>\n<p>HPC researchers are encouraged to install their own R packages if they have not been already installed for the version of R you will be using on HPC.<\/p>\n<p>By default, R will install <em>local<\/em> or <em>user<\/em> packages in your home directory under <strong>~\/R<\/strong>. To avoid filling up your home directory, you must perform a one-time initialization step to change the install location for R packages.<\/p>\n<p>From your home directory, create a new R_packages directory in your project directory. Then create a symbolic link to the R_packages directory and name it &#8220;R&#8221;.<\/p>\n<pre>\r\ncd ~\r\nmkdir \/home\/rcf-proj\/&lt;project&gt;\/&lt;user&gt;\/R_packages\r\nln -s \/home\/rcf-proj\/&lt;project&gt;\/&lt;user&gt;\/R_packages <strong>R<\/strong>\r\n<\/pre>\n<p>A symbolic link appears to be identical to the file or directory it links to. You can see that it is actually a link by typing <strong>ls -l<\/strong>.<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ ls -l R\r\nlrwxr-xr-x  1 ttrojan hpc  Apr 10 R -> \/home\/rcf-proj\/tt\/ttrojan\/R_packages\/\r\n<\/pre>\n<p>Now when you install R packages, R will still place them in <strong>~\/R<\/strong> by default, but the link will reroute the files to the new R_packages directory in your project directory, which has a much higher disk quota.<\/p>\n<h2>Installing R<\/h2>\n<h3>Installing R Packages from CRAN<\/h3>\n<p>After the initialization step, you can install R packages by sourcing the environment setup file for the selected version, starting R, installing the package, and loading the library.<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ source \/usr\/usc\/R\/default\/setup.sh\r\n[ttrojan@hpc-login3]$ R\r\n:\r\n> install.packages(\"&lt;package_name&gt;\")\r\n> library(\"&lt;package_name&gt;\")\r\n<\/pre>\n<p>If you want to use R packages that are stored in other locations, for example, a shared project directory, you must specify the package path, e.g.:<\/p>\n<pre>\r\n> install.packages(\"&lt;package_name&gt;\", lib=\"\/path\/to\/packages\") \r\n> library (&lt;package_name&gt;, lib.loc=\"\/path\/to\/packages\")\r\n<\/pre>\n<p>You can explicitly set a package path using R&#8217;s <em>R_LIBS_USER<\/em> environment variable in your Slurm or environment setup script with a line like the following:<\/p>\n<pre>\r\nexport R_LIBS_USER=\/home\/rcf-proj\/tt\/trojan\/R\/parallel:$R_LIBS_USER\r\n<\/pre>\n<p>You can display the current library paths with <strong>.libPaths()<\/strong>. Note: R packages are version-specific so that a library may exist for one version and not for another.<\/p>\n<pre>\r\n>.libPaths()\r\n[1] \"\/auto\/rcf-proj\/tt\/ttrojan\/R_packages\/x86_64-pc-linux-gnu-library\/3.3\" \r\n[2] \"\/auto\/usc\/R\/3.3.1\/lib64\/R\/library\"\r\n<\/pre>\n<p>To investigate the packages and versions of packages installed under <strong>\/usr\/usc\/R\/default<\/strong> (or under a particular version), use the command <strong>installed.packages<\/strong> and specify the location of the library. <\/p>\n<pre>\r\n> installed.packages(lib.loc=\"\/usr\/usc\/R\/3.5.0\/lib64\/R\/library\") \r\n> installed.packages(lib.loc=\"~\/R\/x86_64-pc-linux-gnu-library\/3.3\")\r\n<\/pre>\n<p>To check if a specific package is installed, use <strong>system.file(package=&#8221;&lt;package_name&gt;&#8221;)<\/strong> or <strong>packageDescription(&#8220;&lt;package_name&gt;&#8221;)<\/strong>, e.g.:<\/p>\n<pre>\r\n> system.file(package=\"parallel\")\r\n[1] \"\/usr\/usc\/R\/3.5.0\/lib64\/R\/library\/parallel\"\r\n\r\n> packageDescription(\"parallel\")\r\nPackage: parallel\r\nVersion: 3.5.0\r\nPriority: base\r\nTitle: Support for Parallel computation in R\r\nAuthor: R Core Team\r\nMaintainer: R Core Team <R-core@r-project.org>\r\nDescription: Support for parallel computation, including by forking\r\n        (taken from package multicore), by sockets (taken from package\r\n        snow) and random-number generation.\r\nLicense: Part of R 3.5.0\r\nImports: tools, compiler\r\nSuggests: methods\r\nEnhances: snow, nws, Rmpi\r\nNeedsCompilation: yes\r\nBuilt: R 3.5.0; x86_64-pc-linux-gnu; 2018-06-22 22:14:04 UTC; unix\r\n\r\n-- File: \/auto\/usc\/R\/3.5.0\/lib64\/R\/library\/parallel\/Meta\/package.rds \r\n<\/pre>\n<h3>Installing R Packages from Other Repositories<\/h3>\n<p>If you want to install packages from non-default repositories, such as BioConductor, use <strong>setRepositories()<\/strong> and supply the number of the repository listed. The next time you type <strong>setRepositories()<\/strong>, you should see a &#8220;+&#8221; next to the new repository. This example was run using R 3.5.0.<\/p>\n<pre>\r\n> setRepositories()\r\nRepositories\r\n\r\n1: + CRAN\r\n2:   BioC software\r\n3:   BioC annotation\r\n4:   BioC experiment\r\n5:   CRAN (extras)\r\n6:   Omegahat\r\n7:   R-Forge\r\n8:   rforge.net\r\n\r\nEnter one or more numbers separated by spaces, or an empty line to cancel\r\n1: <strong>2 3 4<\/strong>\r\n>\r\n> install.packages(\"BiocManager\")\r\n> BiocManager::install(\"BiocParallel\")\r\n> library('BiocParallel')\r\n<\/pre>\n<h2>Running R<\/h2>\n<h3>Running R Interactively<\/h3>\n<p>It is a good idea to test your R program on an &#8220;interactive&#8221; compute node, before submitting a batch (remote) job. The following will request a compute node with 8 CPUs, each with 2GB of memory, for 1 hour and, when the resource is allocated, log you into the node.<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ salloc --ntasks=8 --mem-per-cpu=2g --time=1:00:00\r\nsalloc: Pending job allocation 2377051\r\nsalloc: job 2377051 queued and waiting for resources\r\nsalloc: job 2377051 has been allocated resources\r\nsalloc: Granted job allocation 2377051\r\nsalloc: Waiting for resource configuration\r\nsalloc: Nodes hpc3676 are ready for job\r\n---------- Begin SLURM Prolog ----------\r\nJob ID:        2377051\r\nUsername:      ttrojan\r\nAccountname:   lc_hpcc\r\nName:          sh\r\nPartition:     quick\r\nNodelist:      hpc3676\r\nTasksPerNode:  8\r\nCPUsPerTask:   Default[1]\r\nTMPDIR:        \/tmp\/2377051.quick\r\nSCRATCHDIR:    \/staging\/scratch\/2377051\r\nCluster:       uschpc\r\nHSDA Account:  false\r\n---------- 2018-12-12 17:59:36 ---------\r\n[ttrojan@hpc3676]$ \r\n<\/pre>\n<p>Once you are on a compute node, select the version of R you wish to run from <strong>\/usr\/usc\/R<\/strong>. &#8220;Sourcing&#8221; the script <strong>\/usr\/usc\/R\/&lt;version&gt;\/setup.sh<\/strong> will configure your environment to find (and use) that version of R and Rscript. You can run your program on the command line with Rscript. For example:<\/p>\n<pre>\r\n[ttrojan@hpc3676]$ source \/usr\/usc\/R\/3.5.0\/setup.sh\r\n[ttrojan@hpc3676]$ Rscript hello.R\r\n[1] \"Hello Tommy\"\r\n<\/pre>\n<p>Alternatively, you can run your program within R or RStudio (if you have it installed). If your program is not in the same directory as the one in which you opened R, you will need to either specify a path or set R&#8217;s working directory to the location of the program before you call it.<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ R\r\nR version 3.5.0 (2018-04-23) -- \"Joy in Playing\"\r\n:\r\n# If code in same directory where R was invoked\r\n> source(\"hello.R\")\r\n[1] \"Hello Tommy\"\r\n\r\n# If not, using an absolute path will work\r\n> source(\"\/home\/rcf-proj\/tt\/ttrojan\/R\/hello.R\")\r\n[1] \"Hello Tommy\"\r\n<\/pre>\n<h3>Running R Remotely<\/h3>\n<p>When you have tested your R program and are ready to run it remotely, i.e., in batch mode, you will create a new text file, called a job script, where you will specify the compute resources and commands needed to run your job. The following job script, <strong>myjob.slurm<\/strong>, will request 16 CPUs on a single compute node, each with 2GB of memory, for 4 hours, and will then set up R 3.5.0 and run <strong>myscript.R<\/strong>.<\/p>\n<pre>\r\n#!\/bin\/bash<\/span>\r\n#SBATCH --ntasks=1<\/span>\r\n#SBATCH --cpus-per-task=16<\/span>\r\n#SBATCH --mem-per-cpu=2GB<\/span>\r\n#SBATCH --time=4:00:00<\/span>\r\n#SBATCH --export=none #Ensures job gets a fresh login environment<\/span>\r\n\r\nsource \/usr\/usc\/R\/3.5.0\/setup.sh\r\nRscript myscript.R\r\n<\/pre>\n<p>After your R job script has been created you can use the command <strong>sbatch &lt;job_script&gt;<\/strong> to submit your job:<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ sbatch example_R.slurm\r\nSubmitted batch job 1131075\r\n<\/pre>\n<p>To check on the status of your job use the <strong>squeue -u <<em>username<\/em>><\/strong> command. If you are on a head node, you can use the HPC wrapper, <strong>myqueue<\/strong>.<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ myqueue\r\nJOBID    USER  ACCOUNT  PARTITION  NAME             TASKS  CPUS_PER_TASK  MIN_MEMORY  START_TIME           TIME  TIME_LIMIT  STATE    NODELIST(REASON)\r\n1131075  ttrojan  lc_tt1  quick      example_R  16     1              1G          2018-07-02T11:14:46  1:49  30:00       RUNNING  hpc1046\r\n\r\n[ttrojan@hpc-login3]$ squeue -u ttrojan\r\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n1131095     quick example_R     ttrojan PD       0:00      2 (Resources)\r\n<\/pre>\n<p>By default, all output sent to the console, including error messages and print statements, is directed to a file named &#8220;slurm-%j.out&#8221;, where the &#8220;%j&#8221; is replaced with the job ID number. The file will be generated on the first node of the job allocation.<\/p>\n<p>Any files created by the R program itself will be created as specified by the program.<\/p>\n<h3>Viewing R Plots on HPC<\/h3>\n<p>The following code produces a jpeg file<\/p>\n<pre>\r\n> x <- rnorm(50)\r\n> y <- rnorm(x)\r\n\r\n## If testing interactively with Xforwarding enabled, this command will display an interactive plot\r\n> plot(x,y)\r\n\r\n## If this is a batch job, the following three commands can be used to save your plot to a file.\r\n> png('rplot.png')\r\n> plot(x,y)\r\n> dev.off()\r\n:\r\n> quit()\r\n\r\n## Check that plot was created\r\n$ ls *.png\r\nrplot.png\r\n\r\n## If X-forwarding is enabled, you can use the ImageMagick utility \/usr\/bin\/display to view the plot, or download it to your personal computer and view it there.\r\n$ display rplot.png\r\n<\/pre>\n<h3>Running RStudio on HPC<\/h3>\n<p>Researchers from USC&#8217;s Biostatistics community have developed instructions for installing and running RStudio on HPC. These are available under <a href=\"https:\/\/github.com\/USCbiostats\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\">https:\/\/github.com\/USCbiostats<\/a>.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/USCBiostats\/software-dev\/wiki\/Running-R-on-HPC\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/github.com']);\" target=\"_blank\" rel=\"noopener noreferrer\">https:\/\/github.com\/USCBiostats\/software-dev\/wiki\/Running-RStudio-on-the-HPC<\/a>\n<\/ul>\n<p>Questions about regarding RStudio should be addressed to the Biostatistics GitHub developers and not to HPC staff.<\/p>\n<h3>Reproducibility<\/h3>\n<p>If devtools is installed, you can save the session information for the purposes of reproducibility. <\/p>\n<pre>\r\n> library('devtools')\r\n> session_info()\r\n\u2500 Session info \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n setting  value                       \r\n version  R version 3.5.0 (2018-04-23)\r\n os       CentOS Linux 7 (Core)       \r\n system   x86_64, linux-gnu           \r\n ui       X11                         \r\n language (EN)                        \r\n collate  en_US.UTF-8                 \r\n ctype    en_US.UTF-8                 \r\n tz       US\/Pacific                  \r\n date     2018-12-18                  \r\n\r\n\u2500 Packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n package      * version date       lib source        \r\n assertthat     0.2.0   2017-04-11 [2] CRAN (R 3.5.0)\r\n backports      1.1.3   2018-12-14 [1] CRAN (R 3.5.0)\r\n BiocParallel * 1.16.2  2018-11-28 [1] Bioconductor  \r\n callr          3.1.0   2018-12-10 [1] CRAN (R 3.5.0)\r\n cli            1.0.0   2017-11-05 [2] CRAN (R 3.5.0)\r\n crayon         1.3.4   2017-09-16 [2] CRAN (R 3.5.0)\r\n desc           1.2.0   2018-05-01 [1] CRAN (R 3.5.0)\r\n devtools     * 2.0.1   2018-10-26 [1] CRAN (R 3.5.0)\r\n digest         0.6.18  2018-10-10 [1] CRAN (R 3.5.0)\r\n fs             1.2.6   2018-08-23 [1] CRAN (R 3.5.0)\r\n glue           1.2.0   2017-10-29 [2] CRAN (R 3.5.0)\r\n magrittr       1.5     2014-11-22 [2] CRAN (R 3.5.0)\r\n memoise        1.1.0   2017-04-21 [1] CRAN (R 3.5.0)\r\n pkgbuild       1.0.2   2018-10-16 [1] CRAN (R 3.5.0)\r\n pkgload        1.0.2   2018-10-29 [1] CRAN (R 3.5.0)\r\n prettyunits    1.0.2   2015-07-13 [1] CRAN (R 3.5.0)\r\n processx       3.2.1   2018-12-05 [1] CRAN (R 3.5.0)\r\n ps             1.2.1   2018-11-06 [1] CRAN (R 3.5.0)\r\n R6             2.2.2   2017-06-17 [2] CRAN (R 3.5.0)\r\n Rcpp           0.12.17 2018-05-18 [2] CRAN (R 3.5.0)\r\n remotes        2.0.2   2018-10-30 [1] CRAN (R 3.5.0)\r\n rlang          0.2.1   2018-05-30 [2] CRAN (R 3.5.0)\r\n rprojroot      1.3-2   2018-01-03 [1] CRAN (R 3.5.0)\r\n sessioninfo    1.1.1   2018-11-05 [1] CRAN (R 3.5.0)\r\n snow           0.4-2   2016-10-14 [2] CRAN (R 3.5.0)\r\n usethis      * 1.4.0   2018-08-14 [1] CRAN (R 3.5.0)\r\n withr          2.1.2   2018-03-15 [1] CRAN (R 3.5.0)\r\n\r\n[1] \/auto\/rcf-proj\/ess\/erinshaw\/myRPackages\/x86_64-pc-linux-gnu-library\/3.5\r\n[2] \/auto\/usc\/R\/3.5.0\/lib64\/R\/library\r\n<\/pre>\n<h2>Getting Help<\/h2>\n<p>For assistance with running R on HPC, see our <a href=\"\/getting-help\">Getting Help<\/a> page or send an email to <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>The following is meant to be a brief guide to running R jobs on HPC. For more details, please see the New User Guide or Running a job on HPC using Slurm. Initialization Step (Before Installing R Packages) HPC researchers&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/documentation\/r\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":44,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2973"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=2973"
             }
          ],
          "version-history":[
             {
                "count":6,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2973\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3022,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2973\/revisions\/3022"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/44"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=2973"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":2971,
       "date":"2019-06-03T08:47:41",
       "date_gmt":"2019-06-03T15:47:41",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=2971"
       },
       "modified":"2019-06-07T15:44:26",
       "modified_gmt":"2019-06-07T22:44:26",
       "slug":"python",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/documentation\/python\/",
       "title":{
          "rendered":"Python on HPC"
       },
       "content":{
          "rendered":"<p>The following is meant to be a brief guide to running Python jobs on HPC. For more details, please see the <a href=\"https:\/\/hpcc.usc.edu\/gettingstarted\/\" >New User Guide<\/a> or <a href=\"https:\/\/hpcc.usc.edu\/support\/documentation\/slurm\/\" >Running a job on HPC using Slurm<\/a>.<\/p>\n<p>HPC maintains multiple versions of Python and Anaconda in <strong>\/usr\/usc\/python<\/strong>. The following directions will always refer to a version of Python under <strong>\/usr\/usc\/python<\/strong> \u2014 and not to a version in <strong>\/usr\/bin\/python<\/strong> (if it exists) that may be installed as part of the operating system.<\/p>\n<h2>Managing Python Packages<\/h2>\n<p><strong>Before you run Python packages on HPC<\/strong>, follow the steps in this section to learn how to check which packages are currently installed, create storage space for your Python packages, and how to share packages among project members.<\/p>\n<h3>Pre-Installed Packages<\/h3>\n<p>HPC installs a number of distributed-computing-related packages when it installs a new version of Python. The packages can vary for each version. For version 3.6.0, the <strong>pip3 list<\/strong> command lists the following global packages and dependencies: numpy, scipy, matplotlib, openpyxl, pandas, scikit-image, scikit-learn, pillow, python-igraph, mpi4py, Rpy2, Yapsy, ipython, theano, opencv-python, pycuda, keras, Cython, sparsehash, wheel, and pycairo.<\/p>\n<h3>Storing Python Packages<\/h3>\n<p>HPC researchers are also encouraged to install their own Python packages on HPC (or upgrade those that were pre-installed). By default, Python will install <em>local<\/em> (i.e., <em>user<\/em>) packages in your home directory, in the subdirectory named <strong>.local<\/strong> (the dot in front is part of the name and is required, (e.g., <strong>~\/.local<\/strong>, <strong>\/home\/rcf-40\/ttrojan\/.local<\/strong>). Python will create this directory if it does not already exist.<\/p>\n<h3>Initialization Step<\/h3>\n<p>To avoid filling up the limited disk space in your home directory, you must perform a one-time initialization step to change the installation location for Python packages. First, create a new <strong>Python_packages<\/strong> directory in your project directory (in the example below, this is done from the home directory). Then, create a symbolic link to your new package directory and name it <strong>.local<\/strong>.<\/p>\n<pre>cd ~\r\nmkdir \/home\/rcf-proj\/&lt;<em>project<\/em>&gt;\/&lt;<em>username<\/em>&gt;\/Python_packages\r\nln -s \/home\/rcf-proj\/&lt;<em>project<\/em>&gt;\/&lt;<em>username<\/em>&gt;\/Python_packages .local\r\n<\/pre>\n<p>Where &lt;<em>project<\/em>&gt; is your project name and &lt;<em>username<\/em>&gt; is your username.<\/p>\n<p>A symbolic link appears to be identical to the file or directory it links to. You can see that it is actually a link by typing <strong>ls -la<\/strong>. (The &#8216;a&#8217; is necessary because &#8220;dot&#8221; files are hidden from regular listings.)<\/p>\n<pre>$ ls -la .local\r\nlrwxr-xr-x  1 ttrojan lc_tt1  Apr 10 .local -&gt; \/home\/rcf-proj\/tt1\/ttrojan\/Python_packages\/\r\n<\/pre>\n<p>When you install packages, Python will still place them in <strong>~\/.local<\/strong> by default and the symlink will reroute the files to the <strong>Python_packages\/<\/strong> directory in your project space.<\/p>\n<h3>Sharing Python Packages<\/h3>\n<p>Some research groups may find it convenient to use a shared package directory so that all members can use the exact same packages and conserve their shared disk quota. If your research group wishes to do this, you can create a &#8220;Python_packages&#8221; directory in the group&#8217;s project directory. Each member must then create their own symlink to this directory. Keep in mind that permissions must be set so that the entire group has at least &#8220;read&#8221; permissions for the group&#8217;s directory. Those who will be installing\/upgrading packages will also need &#8220;write&#8221; permissions. <strong>NOTE: ~\/.local<\/strong> can only be symbolically linked to one directory.<\/p>\n<h2>Installing\/Upgrading Packages<\/h2>\n<p>Once you complete the initial steps above, you can install Python packages. First, configure your runtime environment by sourcing the <strong>setup.sh<\/strong> file for the version of Python you want to use. Once you&#8217;ve sourced the setup file, use <strong>Python&#8217;s package installer (PIP)<\/strong> to install packages from the command line.<\/p>\n<p>To perform a new user (local) install of the Python package <em>pytest<\/em>:<\/p>\n<pre>$ source \/usr\/usc\/python\/3.6.0\/setup.sh\r\n$ pip3 install pytest --user \r\n<\/pre>\n<p>To upgrade the currently-installed Python package <em>pytest<\/em>:<\/p>\n<pre>$ pip3 install pytest --user --upgrade\r\n<\/pre>\n<p>To see a list of all installed packages and their current and latest versions:<\/p>\n<pre>$ pip3 list -o --format columns\r\nPackage           Version    Latest      Type \r\n----------------- ---------- ----------- -----\r\nh5py              2.8.0      2.9.0       wheel\r\nmpi4py            2.0.0      3.0.0       sdist\r\n  :\r\n<\/pre>\n<p><strong>NOTE:<\/strong> Python versions 3.X and 2.X have differently named binaries. To invoke Python and pip for Python 3.X, use &#8220;python3&#8221; and &#8220;pip3&#8221;; for Python 2.X, simply use &#8220;python&#8221; and &#8220;pip&#8221;.<\/p>\n<h2>Running Python<\/h2>\n<h3>Running Python Interactively<\/h3>\n<p>It is a good idea to test your Python program on an interactive compute node before submitting a batch (remote) job. The Slurm command <em>salloc<\/em> will request a compute node with 8 CPUs, each with 2GB of memory, for 1 hour and, when the resource is allocated, log you into the node.<\/p>\n<pre>[ttrojan@hpc3676]$ <strong>salloc --ntasks=8 --mem-per-cpu=2g --time=1:00:00<\/strong>\r\nsalloc: Pending job allocation 2377051\r\nsalloc: job 2377051 queued and waiting for resources\r\nsalloc: job 2377051 has been allocated resources\r\nsalloc: Granted job allocation 2377051\r\nsalloc: Waiting for resource configuration\r\nsalloc: Nodes hpc3676 are ready for job\r\n---------- Begin SLURM Prolog ----------\r\nJob ID:        2377051\r\nUsername:      ttrojan\r\nAccountname:   lc_tt1\r\nName:          sh\r\nPartition:     quick\r\nNodelist:      hpc3676\r\nTasksPerNode:  8\r\nCPUsPerTask:   Default[1]\r\nTMPDIR:        \/tmp\/2377051.quick\r\nSCRATCHDIR:    \/staging\/scratch\/2377051\r\nCluster:       uschpc\r\nHSDA Account:  false\r\n---------- 2018-12-12 17:59:36 ---------\r\n[ttrojan@hpc3676]$  \r\n<\/pre>\n<p>Once you are on a compute node, select the version of Python you wish to run from <strong>\/usr\/usc\/python<\/strong>. Now load, or source, the setup script for your selected version to configure your current environment to find and use that version of Python and pip. You can run your program on the command line with the command <strong>python3<\/strong>.<\/p>\n<pre>\r\n$ <strong>source \/usr\/usc\/python\/3.6.0\/setup.sh<\/strong>\r\n$ <strong>python3 hello.py<\/strong>\r\nHello Tommy\r\n<\/pre>\n<p><strong>NOTE:<\/strong> This above script assumes you are running in a bash shell. For a (t)csh shell, source <strong>setup.csh<\/strong>.<\/p>\n<p>Alternatively, you can run your program within Python&#8217;s interpreter. You have to explicitly specify the path of your program.<\/p>\n<pre>$ <strong>python3<\/strong>\r\nPython 3.6.0 (default, Feb 17 2017, 15:36:40) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n# If code in same directory where Python was invoked\r\n&gt;&gt;&gt; exec(open('.\/hello.py').read())\r\nHello Tommy\r\n\r\n# If not, use an absolute path\r\n&gt;&gt;&gt; exec(open('\/home\/rcf-proj\/tt1\/ttrojan\/python\/hello.py').read())\r\nHello Tommy\r\n<\/pre>\n<h3>Running Python Remotely<\/h3>\n<p>Once you are confident that your program will finish without your intervention, you are ready to run it remotely as a batch (non-interactive) job. Batch jobs are submitted to a job scheduler using a text file called a job script, in which you specify the compute resources and commands needed to run your job.<\/p>\n<p>The following job script, <strong>myjob.slurm<\/strong>, will request 16 CPUs on a single compute node, each with 2 gigabytes of memory, for 4 hours. When the job starts, it will then set up Python 3.6.0 and run myprogram.py.<\/p>\n<pre>#!\/bin\/bash\r\n#SBATCH --ntasks=1\r\n#SBATCH --cpus-per-task=16\r\n#SBATCH --mem-per-cpu=2GB\r\n#SBATCH --time=4:00:00\r\n#SBATCH --export=none # Ensures job gets a fresh login environment\r\n\r\nsource \/usr\/usc\/python\/3.6.0\/setup.sh\r\npython3 myprogram.py\r\n<\/pre>\n<p>You can now submit your job for remote processing using Slurm&#8217;s <strong>sbatch <em>jobscript<\/em><\/strong> command.<\/p>\n<pre>$ <strong>sbatch myjob.slurm<\/strong>\r\nSubmitted batch job 1131075\r\n<\/pre>\n<p>To check on the status of your job use the command <strong>squeue -u <em>username<\/em><\/strong>. If you are on a head node, you can use the HPC squeue wrapper <strong>myqueue<\/strong>.<\/p>\n<pre>\r\n$ <strong>squeue -u ttrojan<\/strong>\r\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n1131095     quick example_py     ttrojan PD       0:00      2 (Resources)\r\n\r\n$ <strong>myqueue<\/strong>\r\nJOBID    USER  ACCOUNT  PARTITION  NAME             TASKS  CPUS_PER_TASK  MIN_MEMORY  START_TIME           TIME  TIME_LIMIT  STATE    NODELIST(REASON)\r\n1131075  ttrojan  lc_tt1  quick      example_R  16     1              1G          2018-07-02T11:14:46  1:49  30:00       RUNNING  hpc1046\r\n<\/pre>\n<p>By default, all output sent to the console, including error messages and print statements, is directed to a file named &#8220;slurm-%j.out,&#8221; where the &#8220;%j&#8221; is replaced with the job ID number. The file will be generated on the first node of the job allocation.<\/p>\n<p>Any files created by the Python program itself will be created as specified by the program.<\/p>\n<h2>Getting Help<\/h2>\n<p>For assistance with running Python on HPC, see our <a href=\"\/getting-help\">Getting Help<\/a> page or send an email to <a href=\"mailto:hpc@usc.edu\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>The following is meant to be a brief guide to running Python jobs on HPC. For more details, please see the New User Guide or Running a job on HPC using Slurm. HPC maintains multiple versions of Python and Anaconda&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/documentation\/python\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":44,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2971"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=2971"
             }
          ],
          "version-history":[
             {
                "count":10,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2971\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":3021,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2971\/revisions\/3021"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/44"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=2971"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":2950,
       "date":"2019-04-23T10:50:40",
       "date_gmt":"2019-04-23T17:50:40",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=2950"
       },
       "modified":"2019-04-23T13:57:37",
       "modified_gmt":"2019-04-23T20:57:37",
       "slug":"parallel-matlab-r2018",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/documentation\/matlab\/parallel-matlab-r2018\/",
       "title":{
          "rendered":"Parallel MATLAB (R2018)"
       },
       "content":{
          "rendered":"<p>The following documentation assumes you are using release R2018a or R2018b. We recommend that you use R2018 or higher, going forward. If you must use R2016b, please see the README file in \/usr\/usc\/matlab\/R2016b\/parallel_scripts\/matlab-slurm-master\/README. R2017 releases have not been configured to er under Slurm on HPC. Please send email to <a email=\"hpc@usc.edu\">hpc@usc.edu<\/a> if you need to use another version.<\/p>\n<p>HPC maintains software licenses for MATLAB&#8217;s <a href=\"https:\/\/www.mathworks.com\/help\/distcomp\/index.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">Parallel Computing Toolbox\u2122<\/a> (PCT) and <a href=\"https:\/\/www.mathworks.com\/help\/matlab-parallel-server\/index.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">Parallel Server<\/a> (formerly named the Distributed Computing Server). PCT has support for high-level constructs like parallel for-loops and special array types that let you parallelize code without CUDA or MPI programming. PCT supports [single node] multicore and GPU processing. The Parallel Server lets you do parallel and distributed computation on multi-node clusters. It includes a built-in job scheduler that has been integrated with HPC&#8217;s Slurm resource manager.<\/p>\n<h3>Cluster Configuration<\/h3>\n<p>To run MATLAB on HPC, you will first create a <em>Cluster Profile<\/em>. There are two types of clusters, <em>LOCAL<\/em> and <em>REMOTE<\/em>. A local cluster is sufficient for a single-node job (think of running a multi-core program on your personal computer). A remote cluster is necessary when you want to run MATLAB across multiple compute nodes. <\/p>\n<h5>Local Cluster (Single Node)<\/h5>\n<p>To create a cluster object for a single-node job, add the following lines to your script. <\/p>\n<pre>\r\ncluster=parallel.cluster.Local();\r\ncluster.JobStorageLocation='<em>storage_dir<\/em>';\r\n[a,b]=evalc('system(''nproc --all'')');\r\ncluster.NumWorkers=str2num(a);\r\n<\/pre>\n<p>Where <font face=\"courier\">&#8216;<em>storage_dir<\/em>&#8216;<\/font> is the path to the directory you have created for temporary job storage. We recommend that you make this directory at the top level of your project directory, e.g., \/home\/rcf-proj\/tt1\/trojan\/matlabJobStorage.<\/p>\n<h5>Remote Cluster (Multi-Node)<\/h5>\n<p>To create a cluster object for a multi-node job, add the following lines to your script. Change the Slurm options as desired.<\/p>\n<pre>\r\ncluster = parallel.cluster.Generic;\r\nset(cluster,'JobStorageLocation', '<em>storage_dir<\/em>');\r\nset(cluster,'HasSharedFilesystem', true);\r\nset(cluster,'IntegrationScriptsLocation','<em>scripts_dir<\/em>');\r\ncluster.AdditionalProperties.SlurmArgs='<em>sbatch_args<\/em>';\r\n<\/pre>\n<p>Where <em>storage_dir<\/em>, <em>scripts_dir<\/em>, and <em>sbatch_args<\/em> are defined below.<\/p>\n<table>\n<tr>\n<td>storage_dir<\/td>\n<td>Path to directory for temporary job storage. Create at the top level of your project directory, e.g., \/home\/rcf-proj\/tt1\/ttrojan\/matlabJobStorage <\/td>\n<\/tr>\n<tr>\n<td>scripts_dir<\/td>\n<td>Path to slurm integration scripts, of the form \/usr\/usc\/matlab\/<em>version<\/em>\/SlurmIntegrationScripts<\/td>\n<\/tr>\n<tr>\n<td>sbatch_args<\/td>\n<td>A string containing Slurm SBATCH arguments to pass to the job scheduler. For example:<br \/>\n<font family=\"courier\"><br \/>\n&nbsp;&nbsp;&ndash;&ndash;partition=&lt;<em>queue_name<\/em>&gt;<br \/>\n&nbsp;&nbsp;&ndash;&ndash;account=&lt;<em>account_name<\/em>&gt;<br \/>\n&nbsp;&nbsp;&ndash;&ndash;constraint=&lt;<em>constraint<\/em>&gt;<br \/>\n<\/font><br \/>\nDo not include processor requests like &ndash;&ndash;ntasks and &ndash;&ndash;nodes, MATLAB will do this for you.\n<\/td>\n<\/tr>\n<\/table>\n<p>By configuring a cluster profile in this way, MATLAB will submit a job to Slurm on your behalf when you start up a pool of workers. Slurm integration is supported using MATLAB scripts located in <strong>\/usr\/usc\/matlab\/&lt;<em>version<\/em>&gt;\/SlurmIntegrationScripts\/<\/strong>, starting with version R2018a.<\/p>\n<h3>Starting a Pool of Workers (Labs)<\/h3>\n<p>Once a cluster profile has been configured, the process for starting up a pool of workers for both a LOCAL and REMOTE cluster is the same. MATLAB refers to its workers as &#8220;labs&#8221;; the command &#8220;labindex&#8221; returns the indexes of the workers.<\/p>\n<p>Assuming you have created a cluster object named <em>cluster<\/em> you can start up a pool of workers using the parpool command. <\/p>\n<pre>\r\npool=parpool(cluster,<em>N<\/em>)\r\n<\/pre>\n<p>Where <font face=\"courier\"><em>N<\/em><\/font> is the number of workers to use. If starting a LOCAL cluster, <strong><font face=\"courier\"><em>N<\/em><\/font> must be at least 1 less than the total number of available CPUs<\/strong> because MATLAB needs one CPU to delegate work.<\/p>\n<p>If using a REMOTE cluster, MATLAB will submit a job to the job scheduler that will request enough resources for N workers.<\/p>\n<p>Make sure that you close your parallel pool when you are done with it.<\/p>\n<pre>\r\ndelete(pool)\r\n<\/pre>\n<p>You may receive warnings about improper timezone formatting when you start a pool. These can be safely ignored. To suppress the warnings, add the following line to your job script.<\/p>\n<pre>\r\nexport TZ=America\/Los_Angeles\r\n<\/pre>\n<p>You can use the <strong>cluster.Jobs<\/strong> function to display the status of your jobs.<\/p>\n<pre>\r\n...\r\n>> cluster.NumWorkers=8;\r\n>> pool=parpool(cluster, 7)\r\nStarting parallel pool (parpool) ...\r\nconnected to 7 workers.\r\n...\r\n>> cluster.Jobs\r\n...\r\n         ID           Type        State   FinishDateTime  Username  Tasks\r\n       -----------------------------------------------------------------\r\n    1     2           pool      running                   ttrojan     31\r\n    2     3     concurrent      running                   ttrojan      7\r\n<\/pre>\n<h3> Monitoring Your Job<\/h3>\n<p>MATLAB will submit a job on your behalf when using the REMOTE cluster. You can track its progress like you would with any other job you submit with the command <strong>squeue -u <em>username<\/em><\/strong>. If you are on a head node, you can use the HPC squeue wrapper <strong>myqueue<\/strong>. Note that the job name is generated by MATLAB.<\/p>\n<pre>\r\n$ <strong>squeue -u ttrojan<\/strong>\r\nJOBID   PARTITION     NAME  USER     ST    TIME  NODES  NODELIST(REASON)\r\n2767401  scavenge     Job1  ttrojan  R     0:58      3  hpc[0681-0683]\r\n\r\n$ <strong>myqueue<\/strong>\r\nJOBID    USER  ACCOUNT  PARTITION  NAME  TASKS  CPUS_PER_TASK  MIN_MEMORY  START_TIME           TIME  TIME_LIMIT  STATE    NODELIST(REASON)\r\n2767401  ttrojan  lc_tt1  scavenge   Job1  24     1              1G          2019-02-26T10:17:19  1:20  1:00:00     RUNNING  hpc[0681-0683]\r\n<\/pre>\n<p>To check job information after a job completes you can use Slurm&#8217;s <strong>sacct<\/strong> command.<\/p>\n<pre>\r\n$ <strong>sacct -j &lt;<em>job_id<\/em>&gt; --format=account,partition%10,jobname%20,state,exitcode%4,elapsed%10,start,ntasks%4,nnodes%4,reqcpus,reqmem%6,maxrss%10,maxvmsize%10,nodelist<\/strong>\r\n   Account  Partition              JobName      State Exit    Elapsed               Start NTas NNod  ReqCPUS ReqMem     MaxRSS  MaxVMSize        NodeList \r\n---------- ---------- -------------------- ---------- ---- ---------- ------------------- ---- ---- -------- ------ ---------- ---------- --------------- \r\n   lc_tt1      quick      matlab_launcher  COMPLETED  0:0   00:01:26 2018-12-17T16:58:34         2       32    1Gc                        hpc[1119,1411] \r\n   lc_tt1                           batch  COMPLETED  0:0   00:01:26 2018-12-17T16:58:34    1    1       24    1Gc    600060K    295556K         hpc1119 \r\n   lc_tt1                          extern  COMPLETED  0:0   00:01:26 2018-12-17T16:58:34    2    2       32    1Gc          0    107952K  hpc[1119,1411] \r\n<\/pre>\n<p>Where &lt;<font face=\"courier\"><em>job_id<\/em><\/font>&gt; is the job Id from Slurm.<\/p>\n<h3>HPC Helper Scripts<\/h3>\n<p>HPC has created two helper scripts, <em>get_LOCAL_cluster.m<\/em> and <em>get_SLURM_cluster.m<\/em>, that are described here. You may find it more convenient to use these functions instead of having similar lines of code repeated in your programs.<\/p>\n<h5>Single Node Cluster<\/h5>\n<p>To use MATLAB on a single node (with multiple cores), create the following file, <strong>get_LOCAL_cluster.m<\/strong>, or copy it from \/home\/rcf-proj\/workshop\/matlab\/get_LOCAL_cluster.m to your working directory.<\/p>\n<pre>\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n% get_LOCAL_cluster(<em>storage_dir<\/em>)\r\n% HPC helper function to create a single node MATLAB cluster under Slurm.\r\n% Place this file in MATLAB's search path and call it from your program.\r\n% Or place the lines of the function directly within your program.\r\n%\r\n% Arguments:\r\n%  storage_dir: The path to an *existing* directory where MATLAB can store\r\n%               files, e.g., \"\/home\/rcf-proj\/tt\/ttrojan\/matlab_storage\".\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n\r\nfunction cluster = get_LOCAL_cluster(<em>storage_dir<\/em>)\r\n   cluster=parallel.cluster.Local();\r\n   cluster.JobStorageLocation=<em>storage_dir<\/em>;\r\n   [a,b]=evalc('system(''nproc --all'')');\r\n   cluster.NumWorkers=str2num(a);\r\nend\r\n<\/pre>\n<h5>Multi-Node Cluster<\/h5>\n<p>To use MATLAB across multiple nodes, create the following file, <strong>get_SLURM_cluster.m<\/strong>, or copy it from \/home\/rcf-proj\/workshop\/matlab\/get_SLURM_cluster.m to your working directory.<\/p>\n<pre>\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n% get_SLURM_cluster(<em>storage_dir<\/em>,<em>scripts_dir<\/em>,<em>sbatch_args<\/em>)\r\n% HPC helper function to create a multi-node MATLAB cluster under Slurm.\r\n% Place this file in MATLAB's search path and call it from your program.\r\n% Or place the lines of the function directly within your program.\r\n%\r\n% Arguments:\r\n%  storage_dir: The path to an *existing* directory where MATLAB can store\r\n%               files, e.g., \"\/home\/rcf-proj\/tt\/ttrojan\/matlab_storage\".\r\n%  scripts_dir: The path to the \/usr\/usc\/matlab\/<version>\/SlurmIntegrationScripts\r\n%               directory, e.g., \"\/usr\/usc\/matlab\/R2018a\/SlurmIntegrationScripts\".\r\n%  sbatch_arg:  A string to pass to slurm with all slurm options *except*\r\n%               ntasks, e.g., '--time=12:00:00 --partition=scec --mem-per-cpu=2G'.\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n\r\nfunction cluster = get_SLURM_cluster(<em>storage_dir<\/em>,<em>scripts_dir<\/em>,<em>sbatch_args<\/em>)\r\n   cluster = parallel.cluster.Generic;\r\n   set(cluster,'JobStorageLocation', <em>storage_dir<\/em>);\r\n   set(cluster,'HasSharedFilesystem', true);\r\n   set(cluster,'IntegrationScriptsLocation',<em>scripts_dir<\/em>);\r\n   cluster.AdditionalProperties.SlurmArgs=<em>sbatch_args<\/em>;\r\nend\r\n<\/pre>\n<h3>HPC Example Scripts<\/h3>\n<p>There are a number of example scripts in \/home\/rcf-proj\/workshop\/matlab\/ that have been tested under 2018a. Copy these to your working directory to use. An existing, writable storage directory must be provided for MATLAB. Create the directory and then edit the examples, replacing the test storage directory with your own. <\/p>\n<ul>\n<li>submit.slurm &#8211; a slurm script to batch process the examples\n<li>estimatePi.m &#8211; example that calculates PI (test with increasing number of cores)\n<li>labIndex.m &#8211; simple labindex example (labindex is the number of a worker)\n<li>broadcastReceive.m &#8211; example of SPMD, labindex, and MPI communication\n<li>submitTasks.m &#8211; example of creating job tasks and running them in parallel\n<\/ul>\n<p>You can test the examples interactively or use <strong>sbatch submit.slurm<\/strong> to run them remotely. Edit the submit.slurm script and select the example you would like to run. Change the job and output file names to reflect your choice.<\/p>\n<pre>\r\n$ cat submit.slurm\r\n#!\/bin\/bash\r\n\r\n#SBATCH --ntasks=8\r\n#SBATCH --mem-per-cpu=1g\r\n#SBATCH --time=00:20:00\r\n#SBATCH --export=none\r\n#SBATCH --job-name=matlab-ex1\r\n#SBATCH --output=output-ex1\r\n\r\n#Examples tested using R2018a\r\nsource \/usr\/usc\/matlab\/R2018a\/setup.sh\r\n\r\n#Suppress time warning\r\nexport TZ=America\/Los_Angeles\r\n\r\n#Call an example in this directory\r\nmatlab  -nodisplay -r \"estimatePi\"       #single+multi-node\r\n#matlab -nodisplay -r \"labIndex\"          #multi-node only\r\n#matlab -nodisplay -r \"broadcastReceive\"  #multi-node only\r\n#matlab -nodisplay -r \"submitTasks\"       #single+multi-node\r\n<\/pre>\n<h3>Getting Help<\/h3>\n<p>For additional information on MATLAB, please visit the <a href=\"http:\/\/www.mathworks.com\/access\/helpdesk\/help\/toolbox\/distcomp\/\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">Mathworks help website.<\/a> For any questions related specifically to using Parallel MATLAB on HPC, send an email to <a title=\"hpc@usc.edu\" href=\"mailto:hpc@usc.edu\" target=\"_blank\" rel=\"noopener\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>The following documentation assumes you are using release R2018a or R2018b. We recommend that you use R2018 or higher, going forward. If you must use R2016b, please see the README file in \/usr\/usc\/matlab\/R2016b\/parallel_scripts\/matlab-slurm-master\/README. R2017 releases have not been configured to&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/documentation\/matlab\/parallel-matlab-r2018\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":2946,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2950"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=2950"
             }
          ],
          "version-history":[
             {
                "count":4,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2950\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":2967,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2950\/revisions\/2967"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2946"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=2950"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    },
    {
       "id":2946,
       "date":"2019-04-23T10:44:58",
       "date_gmt":"2019-04-23T17:44:58",
       "guid":{
          "rendered":"https:\/\/hpcc.usc.edu\/?page_id=2946"
       },
       "modified":"2019-04-24T16:10:21",
       "modified_gmt":"2019-04-24T23:10:21",
       "slug":"matlab",
       "status":"publish",
       "type":"page",
       "link":"https:\/\/hpcc.usc.edu\/resources\/documentation\/matlab\/",
       "title":{
          "rendered":"Using MATLAB on HPC"
       },
       "content":{
          "rendered":"<p>The following is meant to be a brief guide to running MATLAB jobs on HPC. For more information on running jobs, please see the <a href=\"https:\/\/hpcc.usc.edu\/gettingstarted\/\" >New User Guide<\/a> or <a href=\"https:\/\/hpcc.usc.edu\/support\/documentation\/slurm\/\" >Running a Job on HPC using Slurm<\/a>.<\/p>\n<h4>MATLAB Paths<\/h4>\n<p>MATLAB uses a search path to locate files. You can explicitly set the search path using the <em>MATLABPATH<\/em> environment variable in your Slurm or environment setup script with a line like the following:<\/p>\n<pre>\r\nexport MATLABPATH=\/home\/rcf-proj\/tt\/ttrojan\/matlab:$MATLABPATH\r\n<\/pre>\n<p>Within a script, use <em>path<\/em> to display MATLAB&#8217;s search path, and <em>addpath()<\/em> to add a new location to the search path.<\/p>\n<pre>\r\n>> path  \r\n>> addpath('\/home\/rcf-proj\/tt\/ttrojan\/matlab')  \r\n>> path\r\n<\/pre>\n<p>See the <a href=\"https:\/\/www.mathworks.com\/help\/matlab\/matlab_env\/what-is-the-matlab-search-path.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">What Is the MATLAB Search Path?<\/a> and <a href=\"https:\/\/www.mathworks.com\/help\/matlab\/matlab_env\/add-folders-to-matlab-search-path-at-startup.html#btpajlw\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">Set the MATLABPATH Environment Variable<\/a> pages on the Mathworks website for more information about MATLAB&#8217;s search path.<\/p>\n<h4>Running MATLAB Interactively<\/h4>\n<p>It is a good idea to test your MATLAB program on an &#8220;interactive&#8221; compute node, before submitting a batch (remote) job. The following will request a compute node with 8 cpus, each with 2GB of memory, for 1 hour and, when the resource is allocated, log you into the node.<\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ salloc --ntasks=8 --mem-per-cpu=2g --time=1:00:00\r\nsalloc: Pending job allocation 2377051\r\nsalloc: job 2377051 queued and waiting for resources\r\nsalloc: job 2377051 has been allocated resources\r\nsalloc: Granted job allocation 2377051\r\nsalloc: Waiting for resource configuration\r\nsalloc: Nodes hpc3676 are ready for job\r\n---------- Begin SLURM Prolog ----------\r\nJob ID:        2377051\r\nUsername:      ttrojan\r\nAccountname:   lc_hpcc\r\nName:          sh\r\nPartition:     quick\r\nNodelist:      hpc3676\r\nTasksPerNode:  8\r\nCPUsPerTask:   Default[1]\r\nTMPDIR:        \/tmp\/2377051.quick\r\nSCRATCHDIR:    \/staging\/scratch\/2377051\r\nCluster:       uschpc\r\nHSDA Account:  false\r\n---------- 2018-12-12 17:59:36 ---------\r\n[ttrojan@hpc3676]$ \r\n<\/pre>\n<p>Once you are on a compute node, select the version of MATLAB you wish to run from \/usr\/usc\/matlab. Now load, or &#8220;source&#8221; the setup script for your selected version to configure your current environment to find and use that version of MATLAB. <\/p>\n<pre>$ source \/usr\/usc\/matlab\/R2018a\/setup.sh<\/pre>\n<p><strong>Note:<\/strong> This assumes you are running in a bash shell. For a (t)csh shell, source setup.csh.<\/p>\n<p>To test an example, create a test file, <strong>hello.m<\/strong> and add the following lines:<\/p>\n<pre>\r\ndisp('Hello Tommy!')\r\nexit\r\n<\/pre>\n<p>You can use the option <strong>&ndash;nodisplay<\/strong> to suppress the graphical interface. Batch jobs run remotely with no GUI. <strong>Note: <\/strong>You cannot suppress the copyright. <\/p>\n<pre>\r\n$ matlab -nodisplay -r hello\r\n\r\n        < M A T L A B (R) >\r\nCopyright 1984-2018 The MathWorks, Inc.\r\nR2018a (9.4.0.813654) 64-bit (glnxa64)\r\n        February 23, 2018\r\n              : \r\nHello Tommy!\r\n<\/pre>\n<p>Alternatively, you can run your program within MATLAB. <\/p>\n<pre>\r\n[ttrojan@hpc-login3]$ matlab -nodisplay\r\n        < M A T L A B (R) >\r\nCopyright 1984-2018 The MathWorks, Inc.\r\nR2018a (9.4.0.813654) 64-bit (glnxa64)\r\n        February 23, 2018\r\n              : \r\n# If code in same directory where MATLAB was invoked\r\n>> hello\r\nHello Tommy!\r\n\r\n# If not, add its path to MATLAB's search path\r\n>> addpath('\/home\/rcf-proj\/tt\/ttrojan\/matlab\/hello')\r\nHello Tommy!\r\n<\/pre>\n<h3>Running MATLAB Remotely<\/h3>\n<p>When you have tested your MATLAB program and are ready to run it remotely, i.e., in batch mode, you will create a new text file, called a job script, where you will specify the compute resources and commands needed to run your job. The following job script, myjob.slurm, will request 16 cpus on a single compute node, each with 2GB of memory, for 4 hours, and will then set up MATLAB R2018a and run myprogram.m.<\/p>\n<pre>#!\/bin\/bash<\/span>\r\n#SBATCH --ntasks=1<\/span>\r\n#SBATCH --cpus-per-task=16<\/span>\r\n#SBATCH --mem-per-cpu=2GB<\/span>\r\n#SBATCH --time=4:00:00<\/span>\r\n#SBATCH --export=none #Ensures job gets a fresh login environment<\/span>\r\n\r\nsource \/usr\/usc\/matlab\/R2018a\/setup.sh\r\nmatlab -nodisplay -r hello\r\n<\/pre>\n<p>You can now submit your job for remote processing using Slurm&#8217;s <strong>sbatch <em>jobscript<\/em><\/strong> command.<\/p>\n<pre>\r\n[ttrojan@hpc-login3 matlab]$ sbatch example_matlab.slurm\r\nSubmitted batch job 1131075\r\n<\/pre>\n<p>To check on the status of your job use the command <strong>squeue -u <em>username<\/em><\/strong>. If you are on a head node, you can use the HPC squeue wrapper <strong>myqueue<\/strong>.<\/p>\n<pre>\r\n$ <strong>squeue -u ttrojan<\/strong>\r\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n1131095     quick example_matlab     ttrojan PD       0:00      2 (Resources)\r\n\r\n$ <strong>myqueue<\/strong>\r\nJOBID    USER  ACCOUNT  PARTITION  NAME             TASKS  CPUS_PER_TASK  MIN_MEMORY  START_TIME           TIME  TIME_LIMIT  STATE    NODELIST(REASON)\r\n1131075  ttrojan  lc_tt1  quick      example_matlab  16     1              1G          2018-07-02T11:14:46  1:49  30:00       RUNNING  hpc1046\r\n<\/pre>\n<p>By default, all output sent to the console, including error messages and print statements, is directed to a file named &#8220;slurm-%j.out&#8221;, where the &#8220;%j&#8221; is replaced with the job ID number. The file will be generated on the first node of the job allocation.<\/p>\n<p>Any files created by the MATLAB program itself will be created as specified by the program.<\/p>\n<h3>Running MATLAB Graphically<\/h3>\n<p>MATLAB can be run graphically, as you would run it on your personal computer. You must log in to a head node with <strong>X11 Forwarding<\/strong> enabled. <\/p>\n<pre>\r\nssh -X <em>username<\/em>@hpc-login.usc.edu\r\n<\/pre>\n<p>This documentation gives only a brief description. The process is explained in detail in the <a href=\"https:\/\/hpcc.usc.edu\/files\/2013\/07\/HPCParallelMatlab9_20170303.pptx.pdf\" >Parallel MATLAB on HPC<\/a> presentation.<\/p>\n<p>To run MATLAB graphically, type the following. The &#8216;&#038;&#8217; runs the program in the background.<\/p>\n<pre>\r\n$ source \/usr\/usc\/matlab\/R2018a\/setup.sh\r\n$ matlab &\r\n<\/pre>\n<h4>MATLAB Licenses<\/h4>\n<p>USC has a university-wide license for MATLAB that includes the Linux versions maintained by USC in \/usr\/usc\/matlab. Additionally, HPC licenses MATLAB&#8217;s Parallel Computing Toolbox and MATLAB Distributed Computing Server for running single and multi node parallel jobs.<\/p>\n<p><strong>The following command displays the number of MATLAB licenses that are in use:<\/strong><\/p>\n<pre>\r\n$ \/usr\/usc\/matlab\/R2016a\/etc\/glnxa64\/lmutil lmstat -a -c \/usr\/usc\/matlab\/R2016a\/licenses\/network.lic | grep MATLAB\r\nUsers of MATLAB:  (Total of 10000 licenses issued;  Total of 16 licenses in use)\r\n  \"MATLAB\" v39, vendor: MLM, expiry: 30-jan-2019\r\nUsers of MATLAB_Distrib_Comp_Engine:  (Total of 1000 licenses issued;  Total of 376 licenses in use)\r\n  \"MATLAB_Distrib_Comp_Engine\" v39, vendor: MLM, expiry: 01-jan-0000\r\n<\/pre>\n<p><strong><br \/>\nYou can search for the MATLAB distributed computing licenses that are checked out to you:<\/strong><\/p>\n<pre>\r\n$ \/usr\/usc\/matlab\/R2016a\/etc\/glnxa64\/lmutil lmstat -a -c \/usr\/usc\/matlab\/R2016a\/licenses\/network.lic | grep -e MATLAB -e Distrib_Comp -e ttrojan\r\nUsers of MATLAB:  (Total of 10000 licenses issued;  Total of 14 licenses in use)\r\n  \"MATLAB\" v39, vendor: MLM, expiry: 30-jan-2019\r\n    ttrojan hpc3613 \/dev\/tty (v36) (hpc-licenses\/28518 121423), start Thu 12\/20 16:09\r\nUsers of Distrib_Computing_Toolbox:  (Total of 10000 licenses issued;  Total of 12 licenses in use)\r\n  \"Distrib_Computing_Toolbox\" v39, vendor: MLM, expiry: 30-jan-2019\r\n    ttrojan hpc3613 \/dev\/tty (v36) (hpc-licenses\/28518 91467), start Thu 12\/20 16:09\r\nUsers of MATLAB_Distrib_Comp_Engine:  (Total of 1000 licenses issued;  Total of 346 licenses in use)\r\n  \"MATLAB_Distrib_Comp_Engine\" v39, vendor: MLM, expiry: 01-jan-0000\r\n    ttrojan hpc3617 \/dev\/tty (v36) (hpc-licenses\/28518 3926), start Thu 12\/20 10:02\r\n    ttrojan hpc3617 \/dev\/tty (v36) (hpc-licenses\/28518 121423), start Thu 12\/20 16:09\r\n    ttrojan hpc3615 \/dev\/tty (v36) (hpc-licenses\/28518 61352), start Thu 12\/20 10:03\r\n    ttrojan hpc3615 \/dev\/tty (v36) (hpc-licenses\/28518 91467), start Thu 12\/20 16:09\r\n<\/pre>\n<h4>Debugging<\/h4>\n<p>There are options for debugging MATLAB. See the <a href=\"https:\/\/www.mathworks.com\/help\/matlab\/ref\/matlablinux.html\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">matlab (Linux)<\/a> page on the Mathworks website.<\/p>\n<h3>Getting Help<\/h3>\n<p>For additional information on MATLAB, please visit the <a href=\"http:\/\/www.mathworks.com\/access\/helpdesk\/help\/toolbox\/distcomp\/\" onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','http:\/\/www.mathworks.com']);\">Mathworks help website.<\/a> For any questions related specifically to using Parallel MATLAB on HPC, send an email to <a title=\"hpc@usc.edu\" href=\"mailto:hpc@usc.edu\" target=\"_blank\" rel=\"noopener\">hpc@usc.edu<\/a>.<\/p>\n",
          "protected":false
       },
       "excerpt":{
          "rendered":"<p>The following is meant to be a brief guide to running MATLAB jobs on HPC. For more information on running jobs, please see the New User Guide or Running a Job on HPC using Slurm. MATLAB Paths MATLAB uses a&#8230; <a class=\"post_read_more\" href=\"https:\/\/hpcc.usc.edu\/resources\/documentation\/matlab\/\" >Read more<\/a><\/p>\n",
          "protected":false
       },
       "author":111,
       "featured_media":0,
       "parent":44,
       "menu_order":0,
       "comment_status":"closed",
       "ping_status":"closed",
       "template":"",
       "meta":[
 
       ],
       "_links":{
          "self":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2946"
             }
          ],
          "collection":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages"
             }
          ],
          "about":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/types\/page"
             }
          ],
          "author":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/users\/111"
             }
          ],
          "replies":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/comments?post=2946"
             }
          ],
          "version-history":[
             {
                "count":6,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2946\/revisions"
             }
          ],
          "predecessor-version":[
             {
                "id":2970,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/2946\/revisions\/2970"
             }
          ],
          "up":[
             {
                "embeddable":true,
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/pages\/44"
             }
          ],
          "wp:attachment":[
             {
                "href":"https:\/\/hpcc.usc.edu\/wp-json\/wp\/v2\/media?parent=2946"
             }
          ],
          "curies":[
             {
                "name":"wp",
                "href":"https:\/\/api.w.org\/{rel}",
                "templated":true
             }
          ]
       }
    }
 ]